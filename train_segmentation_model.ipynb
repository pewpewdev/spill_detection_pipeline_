{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(87790) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting labelme\n",
      "  Downloading labelme-5.8.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting imgviz (from labelme)\n",
      "  Downloading imgviz-1.7.6-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting loguru (from labelme)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: matplotlib in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from labelme) (3.9.3)\n",
      "Collecting natsort>=7.1.0 (from labelme)\n",
      "  Using cached natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from labelme) (2.0.2)\n",
      "Collecting osam>=0.2.3 (from labelme)\n",
      "  Downloading osam-0.2.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pillow>=2.8 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from labelme) (11.0.0)\n",
      "Collecting pyqt5>=5.14.0 (from labelme)\n",
      "  Downloading PyQt5-5.15.11-cp38-abi3-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: pyyaml in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from labelme) (6.0.2)\n",
      "Collecting scikit-image (from labelme)\n",
      "  Downloading scikit_image-0.25.2-cp310-cp310-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: click in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from osam>=0.2.3->labelme) (8.1.7)\n",
      "Requirement already satisfied: gdown in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from osam>=0.2.3->labelme) (5.2.0)\n",
      "Collecting onnxruntime!=1.16.0,!=1.20.0,!=1.20.1,>=1.14.1 (from osam>=0.2.3->labelme)\n",
      "  Downloading onnxruntime-1.21.0-cp310-cp310-macosx_13_0_universal2.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: pydantic in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from osam>=0.2.3->labelme) (2.11.3)\n",
      "Collecting PyQt5-sip<13,>=12.15 (from pyqt5>=5.14.0->labelme)\n",
      "  Downloading PyQt5_sip-12.17.0-cp310-cp310-macosx_10_9_universal2.whl.metadata (472 bytes)\n",
      "Collecting PyQt5-Qt5<5.16.0,>=5.15.2 (from pyqt5>=5.14.0->labelme)\n",
      "  Downloading PyQt5_Qt5-5.15.16-py3-none-macosx_11_0_arm64.whl.metadata (536 bytes)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from matplotlib->labelme) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from matplotlib->labelme) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from matplotlib->labelme) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from matplotlib->labelme) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from matplotlib->labelme) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from matplotlib->labelme) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from matplotlib->labelme) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.11.4 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from scikit-image->labelme) (1.14.1)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from scikit-image->labelme) (3.4.2)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image->labelme)\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->labelme)\n",
      "  Downloading tifffile-2025.3.30-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->labelme)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting coloredlogs (from onnxruntime!=1.16.0,!=1.20.0,!=1.20.1,>=1.14.1->osam>=0.2.3->labelme)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from onnxruntime!=1.16.0,!=1.20.0,!=1.20.1,>=1.14.1->osam>=0.2.3->labelme) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from onnxruntime!=1.16.0,!=1.20.0,!=1.20.1,>=1.14.1->osam>=0.2.3->labelme) (5.29.1)\n",
      "Requirement already satisfied: sympy in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from onnxruntime!=1.16.0,!=1.20.0,!=1.20.1,>=1.14.1->osam>=0.2.3->labelme) (1.13.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->labelme) (1.17.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from gdown->osam>=0.2.3->labelme) (4.12.3)\n",
      "Requirement already satisfied: filelock in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from gdown->osam>=0.2.3->labelme) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from gdown->osam>=0.2.3->labelme) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from gdown->osam>=0.2.3->labelme) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from pydantic->osam>=0.2.3->labelme) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from pydantic->osam>=0.2.3->labelme) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from pydantic->osam>=0.2.3->labelme) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from pydantic->osam>=0.2.3->labelme) (0.4.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from beautifulsoup4->gdown->osam>=0.2.3->labelme) (2.6)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime!=1.16.0,!=1.20.0,!=1.20.1,>=1.14.1->osam>=0.2.3->labelme)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from requests[socks]->gdown->osam>=0.2.3->labelme) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from requests[socks]->gdown->osam>=0.2.3->labelme) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from requests[socks]->gdown->osam>=0.2.3->labelme) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from requests[socks]->gdown->osam>=0.2.3->labelme) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from requests[socks]->gdown->osam>=0.2.3->labelme) (1.7.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/cilvosimon/Downloads/Live_face_recognition/face_env/lib/python3.10/site-packages (from sympy->onnxruntime!=1.16.0,!=1.20.0,!=1.20.1,>=1.14.1->osam>=0.2.3->labelme) (1.3.0)\n",
      "Downloading labelme-5.8.1-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hUsing cached natsort-8.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading osam-0.2.3-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyQt5-5.15.11-cp38-abi3-macosx_11_0_arm64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading imgviz-1.7.6-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Downloading scikit_image-0.25.2-cp310-cp310-macosx_12_0_arm64.whl (13.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading onnxruntime-1.21.0-cp310-cp310-macosx_13_0_universal2.whl (33.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.6/33.6 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyQt5_Qt5-5.15.16-py3-none-macosx_11_0_arm64.whl (37.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.1/37.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyQt5_sip-12.17.0-cp310-cp310-macosx_10_9_universal2.whl (122 kB)\n",
      "Downloading tifffile-2025.3.30-py3-none-any.whl (226 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Installing collected packages: PyQt5-Qt5, tifffile, PyQt5-sip, natsort, loguru, lazy-loader, imageio, humanfriendly, scikit-image, pyqt5, coloredlogs, onnxruntime, imgviz, osam, labelme\n",
      "Successfully installed PyQt5-Qt5-5.15.16 PyQt5-sip-12.17.0 coloredlogs-15.0.1 humanfriendly-10.0 imageio-2.37.0 imgviz-1.7.6 labelme-5.8.1 lazy-loader-0.4 loguru-0.7.3 natsort-8.4.0 onnxruntime-1.21.0 osam-0.2.3 pyqt5-5.15.11 scikit-image-0.25.2 tifffile-2025.3.30\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install labelme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. spills are annotated with polygon in labelme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. polygon annotations are saved in json format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. From json annotation binary mask is created using below code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conversion complete. Check 'images' and 'masks' folders.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from labelme import utils\n",
    "\n",
    "# Paths\n",
    "json_dir = \"image_seg/label_json\"\n",
    "output_img_dir = \"image_seg/images\"\n",
    "output_mask_dir = \"image_seg/masks\"\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(output_img_dir, exist_ok=True)\n",
    "os.makedirs(output_mask_dir, exist_ok=True)\n",
    "\n",
    "# Loop through all JSON files\n",
    "for filename in os.listdir(json_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        json_path = os.path.join(json_dir, filename)\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Decode image\n",
    "        image = utils.img_b64_to_arr(data[\"imageData\"])\n",
    "        \n",
    "        # Create label mask\n",
    "        label_name_to_value = {\"_background_\": 0}\n",
    "        for shape in data[\"shapes\"]:\n",
    "            label_name = shape[\"label\"]\n",
    "            if label_name not in label_name_to_value:\n",
    "                label_name_to_value[label_name] = len(label_name_to_value)\n",
    "\n",
    "        lbl, _ = utils.shapes_to_label(image.shape, data[\"shapes\"], label_name_to_value)\n",
    "\n",
    "        # Save original image\n",
    "        img_name = filename.replace(\".json\", \".png\")\n",
    "        Image.fromarray(image).save(os.path.join(output_img_dir, img_name))\n",
    "\n",
    "        # Save corresponding mask\n",
    "        Image.fromarray(lbl.astype(np.uint8)).save(os.path.join(output_mask_dir, img_name))\n",
    "\n",
    "print(\"✅ Conversion complete. Check 'images' and 'masks' folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "images_folder = 'image_seg/images'\n",
    "masks_folder = 'image_seg/masks'\n",
    "\n",
    "# Get the list of images and masks\n",
    "images = os.listdir(images_folder)\n",
    "masks = os.listdir(masks_folder)\n",
    "\n",
    "# Check if each image has a corresponding mask\n",
    "for image in images:\n",
    "    image_name, _ = os.path.splitext(image)\n",
    "    mask_name = image_name + '.png'  # Masks have the same name as images\n",
    "    \n",
    "    if mask_name not in masks:\n",
    "        print(f\"❌ Warning: No mask found for {image}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. U-Net segementation model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 12s/step - accuracy: 0.8919 - loss: 0.5569 - val_accuracy: 0.9157 - val_loss: 0.4019\n",
      "Epoch 2/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 14s/step - accuracy: 0.8835 - loss: 0.4333 - val_accuracy: 0.9157 - val_loss: 0.3264\n",
      "Epoch 3/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 16s/step - accuracy: 0.9007 - loss: 0.3601 - val_accuracy: 0.9157 - val_loss: 0.3154\n",
      "Epoch 4/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 14s/step - accuracy: 0.8996 - loss: 0.3478 - val_accuracy: 0.9157 - val_loss: 0.3034\n",
      "Epoch 5/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 15s/step - accuracy: 0.8820 - loss: 0.3539 - val_accuracy: 0.9157 - val_loss: 0.2818\n",
      "Epoch 6/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 14s/step - accuracy: 0.8805 - loss: 0.3441 - val_accuracy: 0.9157 - val_loss: 0.2925\n",
      "Epoch 7/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 13s/step - accuracy: 0.8876 - loss: 0.3459 - val_accuracy: 0.9157 - val_loss: 0.2749\n",
      "Epoch 8/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 15s/step - accuracy: 0.8826 - loss: 0.3283 - val_accuracy: 0.9157 - val_loss: 0.2859\n",
      "Epoch 9/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 13s/step - accuracy: 0.8875 - loss: 0.3092 - val_accuracy: 0.9157 - val_loss: 0.2711\n",
      "Epoch 10/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 13s/step - accuracy: 0.8997 - loss: 0.2936 - val_accuracy: 0.9157 - val_loss: 0.2698\n",
      "Epoch 11/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 13s/step - accuracy: 0.8828 - loss: 0.3093 - val_accuracy: 0.9157 - val_loss: 0.2689\n",
      "Epoch 12/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 13s/step - accuracy: 0.8958 - loss: 0.2901 - val_accuracy: 0.9157 - val_loss: 0.2675\n",
      "Epoch 13/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 13s/step - accuracy: 0.9026 - loss: 0.2830 - val_accuracy: 0.9095 - val_loss: 0.2733\n",
      "Epoch 14/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 14s/step - accuracy: 0.9162 - loss: 0.2655 - val_accuracy: 0.9069 - val_loss: 0.2683\n",
      "Epoch 15/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 14s/step - accuracy: 0.9089 - loss: 0.2870 - val_accuracy: 0.9076 - val_loss: 0.2735\n",
      "Epoch 16/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 13s/step - accuracy: 0.9183 - loss: 0.2703 - val_accuracy: 0.9053 - val_loss: 0.2745\n",
      "Epoch 17/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 13s/step - accuracy: 0.9146 - loss: 0.2651 - val_accuracy: 0.9023 - val_loss: 0.2716\n",
      "Epoch 18/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 12s/step - accuracy: 0.8696 - loss: 0.7853 - val_accuracy: 0.9157 - val_loss: 0.3163\n",
      "Epoch 19/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 12s/step - accuracy: 0.8855 - loss: 0.3696 - val_accuracy: 0.9157 - val_loss: 0.3112\n",
      "Epoch 20/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 13s/step - accuracy: 0.8940 - loss: 0.3422 - val_accuracy: 0.9157 - val_loss: 0.3055\n",
      "Epoch 21/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 15s/step - accuracy: 0.8972 - loss: 0.3251 - val_accuracy: 0.9157 - val_loss: 0.2916\n",
      "Epoch 22/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 12s/step - accuracy: 0.8977 - loss: 0.3053 - val_accuracy: 0.9157 - val_loss: 0.2875\n",
      "Epoch 23/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 14s/step - accuracy: 0.8859 - loss: 0.3219 - val_accuracy: 0.9157 - val_loss: 0.2869\n",
      "Epoch 24/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 12s/step - accuracy: 0.8788 - loss: 0.3595 - val_accuracy: 0.9123 - val_loss: 0.2893\n",
      "Epoch 25/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 12s/step - accuracy: 0.8559 - loss: 0.3625 - val_accuracy: 0.9157 - val_loss: 0.2779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "IMAGE_DIR = \"image_seg/images\"\n",
    "MASK_DIR = \"image_seg/masks\"\n",
    "IMG_SIZE = (256, 256)  # Resize everything to this\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 25\n",
    "\n",
    "# DATA GENERATOR \n",
    "class SpillDataset(Sequence):\n",
    "    def __init__(self, image_filenames, image_dir, mask_dir, batch_size, img_size):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_filenames) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_files = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_imgs = []\n",
    "        batch_masks = []\n",
    "\n",
    "        for fname in batch_files:\n",
    "            img = cv2.imread(os.path.join(self.image_dir, fname))\n",
    "            img = cv2.resize(img, self.img_size)\n",
    "            img = img / 255.0\n",
    "\n",
    "            mask = cv2.imread(os.path.join(self.mask_dir, fname), cv2.IMREAD_GRAYSCALE)\n",
    "            mask = cv2.resize(mask, self.img_size)\n",
    "            mask = (mask > 0).astype(np.float32)  # Binary mask\n",
    "\n",
    "            batch_imgs.append(img)\n",
    "            batch_masks.append(np.expand_dims(mask, axis=-1))\n",
    "\n",
    "        return np.array(batch_imgs), np.array(batch_masks)\n",
    "\n",
    "#  U-NET ARCHITECTURE \n",
    "def build_unet(input_size=(256, 256, 3)):\n",
    "    inputs = layers.Input(input_size)\n",
    "\n",
    "    def conv_block(x, filters):\n",
    "        x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "        x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "        return x\n",
    "\n",
    "    def encoder_block(x, filters):\n",
    "        f = conv_block(x, filters)\n",
    "        p = layers.MaxPooling2D((2, 2))(f)\n",
    "        return f, p\n",
    "\n",
    "    def decoder_block(x, skip, filters):\n",
    "        x = layers.Conv2DTranspose(filters, 2, strides=2, padding=\"same\")(x)\n",
    "        x = layers.concatenate([x, skip])\n",
    "        x = conv_block(x, filters)\n",
    "        return x\n",
    "\n",
    "    f1, p1 = encoder_block(inputs, 64)\n",
    "    f2, p2 = encoder_block(p1, 128)\n",
    "    f3, p3 = encoder_block(p2, 256)\n",
    "    f4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    bottleneck = conv_block(p4, 1024)\n",
    "\n",
    "    d1 = decoder_block(bottleneck, f4, 512)\n",
    "    d2 = decoder_block(d1, f3, 256)\n",
    "    d3 = decoder_block(d2, f2, 128)\n",
    "    d4 = decoder_block(d3, f1, 64)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation=\"sigmoid\")(d4)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# --- SPLIT DATA ---\n",
    "all_files = os.listdir(IMAGE_DIR)\n",
    "train_files, val_files = train_test_split(all_files, test_size=0.2, random_state=42)\n",
    "\n",
    "train_gen = SpillDataset(train_files, IMAGE_DIR, MASK_DIR, BATCH_SIZE, IMG_SIZE)\n",
    "val_gen = SpillDataset(val_files, IMAGE_DIR, MASK_DIR, BATCH_SIZE, IMG_SIZE)\n",
    "\n",
    "# --- COMPILE AND TRAIN ---\n",
    "model = build_unet(input_size=(256, 256, 3))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    validation_data=val_gen,\n",
    "                    epochs=EPOCHS)\n",
    "\n",
    "\n",
    "model.save(\"spill_unet_model.h5\")\n",
    "\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Now inferencing the result with first yolo model.\n",
    "# Then cropping the bounding box of detected spill given as input to the U-net model. \n",
    "# U-net model will segemnt the spill region and calculate the center point of the segmentated region to give accurate location of the spill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/14_aug.jpg: 640x640 1 spill, 298.6ms\n",
      "Speed: 4.3ms preprocess, 298.6ms inference, 11.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 539ms/step\n",
      "✅ Processed and saved 14_aug.jpg.\n",
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/50_aug.jpg: 640x640 1 spill, 341.7ms\n",
      "Speed: 0.9ms preprocess, 341.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step\n",
      "✅ Processed and saved 50_aug.jpg.\n",
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/40_aug.jpg: 640x640 1 spill, 265.4ms\n",
      "Speed: 0.9ms preprocess, 265.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step\n",
      "✅ Processed and saved 40_aug.jpg.\n",
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/17.jpg: 640x480 1 spill, 261.4ms\n",
      "Speed: 3.6ms preprocess, 261.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step\n",
      "✅ Processed and saved 17.jpg.\n",
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/13.png: 512x640 1 spill, 254.2ms\n",
      "Speed: 1.3ms preprocess, 254.2ms inference, 0.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 533ms/step\n",
      "✅ Processed and saved 13.png.\n",
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/38_aug.jpg: 640x640 2 spills, 315.7ms\n",
      "Speed: 1.3ms preprocess, 315.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 435ms/step\n",
      "✅ Processed and saved 38_aug.jpg.\n",
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/33.jpg: 640x384 1 spill, 247.0ms\n",
      "Speed: 1.9ms preprocess, 247.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step\n",
      "✅ Processed and saved 33.jpg.\n",
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/9_aug.jpg: 640x640 2 spills, 334.2ms\n",
      "Speed: 0.9ms preprocess, 334.2ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step\n",
      "✅ Processed and saved 9_aug.jpg.\n",
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/18.jpg: 640x480 1 spill, 203.6ms\n",
      "Speed: 9.9ms preprocess, 203.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346ms/step\n",
      "✅ Processed and saved 18.jpg.\n",
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/25.jpg: 640x480 3 spills, 259.1ms\n",
      "Speed: 2.3ms preprocess, 259.1ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step\n",
      "✅ Processed and saved 25.jpg.\n",
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/27_aug.jpg: 640x640 1 spill, 298.2ms\n",
      "Speed: 1.1ms preprocess, 298.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step\n",
      "✅ Processed and saved 27_aug.jpg.\n",
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/43.jpg: 480x640 2 spills, 239.3ms\n",
      "Speed: 1.1ms preprocess, 239.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 549ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step\n",
      "✅ Processed and saved 43.jpg.\n",
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/41.jpg: 640x480 1 spill, 298.2ms\n",
      "Speed: 3.2ms preprocess, 298.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475ms/step\n",
      "✅ Processed and saved 41.jpg.\n",
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/44.jpg: 480x640 1 spill, 425.1ms\n",
      "Speed: 8.4ms preprocess, 425.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609ms/step\n",
      "✅ Processed and saved 44.jpg.\n",
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/3.jpg: 640x640 1 spill, 418.4ms\n",
      "Speed: 2.3ms preprocess, 418.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 514ms/step\n",
      "✅ Processed and saved 3.jpg.\n",
      "\n",
      "image 1/1 /Users/cilvosimon/Desktop/data_files/images_/val/47.jpg: 480x640 1 spill, 256.6ms\n",
      "Speed: 1.6ms preprocess, 256.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step\n",
      "✅ Processed and saved 47.jpg.\n",
      "✅ Final images saved in /Users/cilvosimon/Desktop/data_files/test_output_images3/\n",
      "✅ Spill location coordinates saved in /Users/cilvosimon/Desktop/data_files/test_output_coordinates2.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tensorflow.keras.models import load_model\n",
    "import csv\n",
    "\n",
    "\n",
    "yolo_model_path = 'yolov8m_finetune_wt/yolov8m.pt'\n",
    "unet_model_path = 'spill_unet_model.h5'\n",
    "input_image_dir = 'images_/val'  \n",
    "output_image_dir = 'final_test_output_images/'  \n",
    "csv_file_path = 'test_output_coordinates.csv'  \n",
    "\n",
    "\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "\n",
    "# LOAD MODELS \n",
    "yolo_model = YOLO(yolo_model_path)\n",
    "unet_model = load_model(unet_model_path)\n",
    "\n",
    "\n",
    "def get_center_of_mask(mask):\n",
    "    coords = cv2.findNonZero((mask > 0.5).astype(np.uint8))\n",
    "    if coords is not None:\n",
    "        x, y, w, h = cv2.boundingRect(coords)\n",
    "        cx = x + w // 2\n",
    "        cy = y + h // 2\n",
    "        return (cx, cy)\n",
    "    return None\n",
    "\n",
    "#CSV SETUP\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['image_name', 'spill_location_x', 'spill_location_y'])  # Column headers\n",
    "\n",
    "#PROCESS ALL IMAGES IN TESTING DIRECTORY \n",
    "for image_name in os.listdir(input_image_dir):\n",
    "    # Check if the file is an image (can be modified to suit file types)\n",
    "    if image_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(input_image_dir, image_name)\n",
    "        \n",
    "        # LOAD IMAGE \n",
    "        image = cv2.imread(image_path)\n",
    "        original = image.copy()\n",
    "        h_orig, w_orig = image.shape[:2]\n",
    "\n",
    "        #  YOLO INFERENCE \n",
    "        results = yolo_model(image_path, conf=0.2)[0]\n",
    "\n",
    "        # APPLY NON-MAXIMUM SUPPRESSION (NMS)\n",
    "        # Filter out detections based on confidence threshold\n",
    "        results = results.cpu()  # Move to CPU for further processing\n",
    "        boxes = results.boxes.xyxy  # Bounding box coordinates (x1, y1, x2, y2)\n",
    "        scores = results.boxes.conf  # Confidence scores\n",
    "        labels = results.boxes.cls  # Class labels\n",
    "\n",
    "        # NMS operation\n",
    "        nms_indices = cv2.dnn.NMSBoxes(boxes.tolist(), scores.tolist(), score_threshold=0.2, nms_threshold=0.4)\n",
    "\n",
    "        # If no boxes are selected, continue with next image\n",
    "        if len(nms_indices) == 0:\n",
    "            print(f\"⚠️ No valid boxes after NMS in {image_name}.\")\n",
    "            continue\n",
    "\n",
    "        #  PROCESS ALL DETECTED BOXES AFTER NMS\n",
    "        for i in nms_indices.flatten():\n",
    "            box = boxes[i]\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cropped = image[y1:y2, x1:x2]\n",
    "\n",
    "            #  U-NET PREPROCESS \n",
    "            cropped_resized = cv2.resize(cropped, (256, 256))\n",
    "            input_tensor = cropped_resized / 255.0  # Normalize\n",
    "            input_tensor = np.expand_dims(input_tensor, axis=0)\n",
    "\n",
    "            #  PREDICT SEGMENTATION \n",
    "            pred_mask = unet_model.predict(input_tensor)[0, :, :, 0]\n",
    "\n",
    "            #  RESIZE MASK BACK TO CROP SIZE\n",
    "            pred_mask_resized = cv2.resize(pred_mask, (x2 - x1, y2 - y1))\n",
    "\n",
    "            #  CREATE BINARY MASK USING LOWER THRESHOLD \n",
    "            binary_mask = (pred_mask_resized > 0.1).astype(np.uint8)  # Lowered threshold\n",
    "\n",
    "            #  FIND CENTER OF MASK \n",
    "            center = get_center_of_mask(binary_mask)\n",
    "            if center:\n",
    "                cx, cy = center\n",
    "                cx += x1\n",
    "                cy += y1\n",
    "\n",
    "                #  DRAW YOLO BOUNDING BOX AND SPILL LOCATION \n",
    "                cv2.rectangle(original, (x1, y1), (x2, y2), (0, 255, 0), 4)  # Bounding box with thickness of 4\n",
    "                cv2.circle(original, (cx, cy), 10, (0, 0, 255), -1)  # Larger red circle for center\n",
    "                cv2.putText(original, f\"spill_location: ({cx}, {cy})\", (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)  # Larger text size and thickness\n",
    "\n",
    "                #  SAVE SPILL LOCATION COORDINATES TO CSV \n",
    "                with open(csv_file_path, mode='a', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow([image_name, cx, cy])\n",
    "\n",
    "        #  SAVE FINAL IMAGE \n",
    "        output_image_path = os.path.join(output_image_dir, f\"output_{image_name}\")\n",
    "        cv2.imwrite(output_image_path, original)  \n",
    "\n",
    "        print(f\"✅ Processed and saved {image_name}.\")\n",
    "\n",
    "print(f\"✅ Final images saved in {output_image_dir}\")\n",
    "print(f\"✅ Spill location coordinates saved in {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
